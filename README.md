# ScienceNOW
ScienceNOW: Topic Modelling with Arxiv e-prints for Trend detection.

Installation: 
The easiest way to get started with ScienceNOW is installing the package locally. Set up a virtual environment in which you want to install the package and all dependencies, python 3.10 is recommended.  
```bash
git clone https://github.com/benearnthof/ScienceNOW.git
cd ScienceNOW
pip install -e .
```
To install everything as an editable package.

To train and evaluate topic models, and extract trends from Arxiv Preprints you will need to download the data from the [Open Archives Initiative](https://www.openarchives.org/) (OAI) first. This can be done with  
[https://github.com/mattbierbaum/arxiv-public-datasets/tree/master](https://github.com/mattbierbaum/arxiv-public-datasets/tree/master)   
But is very slow as the OAI limits the amount of metadata one can download to 1000 entries every 10 seconds. Downloading the entire Arxiv Preprint metadata from scratch takes over 10 hours so the data needed to reproduce our experiments are provided via Google Drive:  
[https://drive.google.com/drive/folders/1xhLDDFwJauVH5ijRY94xjVRaChc5g5EO?usp=drive_link](https://drive.google.com/drive/folders/1xhLDDFwJauVH5ijRY94xjVRaChc5g5EO?usp=drive_link)   

There you will find most of the files needed to begin topic modelling:  
* `ARXIV_SNAPSHOT.json` A file containing the metadata of all Arxiv preprints up including the 13th of November 2023.
* `arxiv-metadata-oai-2023-11-13.json.gz` The OAI snapshot generated by downloading the Arxiv Preprint metadata. (Only needed if you wish to update your data at a later point in time.)
* A `.txt` file containint the resumptionToken necessary to do so
* `embeddings.npy` A numpy array obtained by encoding the preprocessed preprint abstracts with a sentence transformer. 
* `reduced_embeddings.npy` A numpy array obtained by using `cuML.manifold.umap` to compress the sentence transformer embeddings down to 5 dimensional vectors.
* `arxiv_df.feather` A pandas DataFrame which contains the data used for topic modelling after all preprocessing steps like filtering and dimensionality reduction are done, stored as `.feather` to drastically speed up loading of 2.3 million preprints to memory
* `taxonomy.txt` A text file containing the mapping from the Arxiv labeling taxonomy to plaintext labels. (cs.CL: Computation and Language, etc.)

Download the files and adjust the respective paths in `secrets.yaml`. It is recommended to leave all these files in a single folder since they will only be written to if the dataset is updated with `sciencenow.data.update_arxiv_snapshot.py`.  

Updating the OAI Snapshot can be done by running:  
```bash
python sciencenow/data/update_arxiv_snapshot.py
```
After you've added the necessary paths to `secrets.yaml` so the update script will find the pre-downloaded snapshot and will be able to continue requesting papers added to the Arxiv after the 13th of November 2023, instead of building a new snapshot from scratch.  

Setting up the project config:  
As mentioned above, to update the OAI snapshot and train models with this package you will need to adjust some parameters in `sciencenow/config/secrets.yaml`.
Navigate from the root of the project directory to the corresponding folder and adjust the following lines in the file:  

```yaml
ROOT: "absolute path to the Project Root e.g.: ./ScienceNOW/"
ARXIV_SNAPSHOT: "absolute path to the metadata snapshot e.g.: /arxiv-public-datasets/arxiv-data/arxiv-metadata-oai-2023-11-13.json"
EMBEDDINGS: "absolute path to the precomputed abstract sentence embeddings e.g.: /arxiv-public-datasets/arxiv-data/embeddings.npy"
REDUCED_EMBEDDINGS: absolute path to the precomputed reduced embeddings e.g.: /arxiv-public-datasets/arxiv-data/reduced_embeddings.npy"
FEATHER_PATH: "absolute path to the preprocessed metadata in .feather format e.g.:/arxiv-public-datasets/arxiv-data/arxiv_df.feather"
TAXONOMY_PATH: "absolute path to the label taxonomy for semisupervised models e.g.: /taxonomy.txt"
EVAL_ROOT: "absolute path to the directory in which evaluation results should be stored e.g.: /tm_evaluation/"
VOCAB_PATH: "absolute path to the file where the vocabulary for evaluation will be stored e.g.: /tm_evaluation/vocab.txt"
CORPUS_PATH: "absolute path to the file where the corpus used for evaluation will be stored e.g.: /tm_evaluation/corpus.tsv"
SENTENCE_MODEL: "sentence transformer used to generate EMBEDDINGS e.g.: all-MiniLM-L6-v2" 
UMAP_NEIGHBORS: 15 umap parameters
UMAP_COMPONENTS: 5
UMAP_METRIC: "cosine"
VOCAB_THRESHOLD: 15 # Threshold used to avoid out of memory errors during evaluation
TM_VOCAB_PATH: "absolute path to the topic model vocab (distinct from evaluation vocab) e.g.: /tm_evaluation/tm_vocab.txt"
TM_TARGET_ROOT: "absolute path to directory where trained topic model should be written to disk e.g.: /tm_evaluation/"
```

We will go over each of these parameters (and other training hyperparameters) in detail, for now let's set up the data we wish to analyze.  

### Setting up the Arxiv Snapshot:  
After downloading the Arxiv Snapshot from the OAI or from the provided Google Drive, make sure to set the respective paths in `secrets.yaml` to the correct locations.  
If you wish to perform topic modelling on large subsets of the Arxiv metadata it is recommended to install (https://github.com/rapidsai/cuml)[https://github.com/rapidsai/cuml] as the parallel implementation of UMAP runs orders of magnitude faster than the default CPU implementation available in the umap python library.  

After updating the OAI snapshot, you need to rerun the preprocessing steps to convert the data into a dataframe and save it as a `.feather` file for faster loading.  
To do so, delete the old `arxiv_df.feather` file and do the following:  
```python
from sciencenow.data.arxivprocessor import ArxivProcessor
processor = ArxivProcessor(sort_by_date=True)
processor.load_snapshot()
```
The `load_snapshot` method will load the preprocessed data frame present at the `FEATHER_PATH` location specified in `secrets.yaml`. If no such file is present, it will read the OAI snapshot in the `ARXIV_SNAPSHOT` location and do the following:  
* Remove any duplicate entries by ID
* Extract the date the first version of each respective preprint was uploaded and parse it as `datetime`.
* Preprocess the abstracts (removal of newline characters).
* Serializing the dataframe as a `.feather` file to disk.

This will cut down preprocessing times for 2.7 million abstracts to a couple of seconds of loading the data frame from disk.  

### Training and Evaluating Topic Models: 

The main goal of this project is the detection of trends or emerging topics in Arxiv preprints. For this purpose we utilize the time stamps present in the metadata of each preprint. While every preprint comes equipped with potentially multiple different time stamps, the `v1_datetime` corresponds to the first time the preprint was added to the Arxiv, which, like we will discuss below, allows us to order all preprints by their distinct timestamps and bin them by day, week, month, or year. The `v1_datetime` does not match the actual publication date of the respective articles but this does not impact our topic models much since on one hand, most preprints do remain preprints forever, and on the other, the few preprints that do end up getting accepted at journals may get published multiple times in multiple journals which may further muddy the waters in respect to picking the "correct" publication date. Despite these slight discrepancies in the way articles are published and updated on the Arxiv, we are still able to find trends and patterns in the way preprints of certain domains are added to the Arxiv, corresponding to publication deadlines of various journals. Some examples are discussed later on.  



